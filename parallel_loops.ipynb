{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "from time import sleep\n",
    "\n",
    "import time\n",
    "# import pptk\n",
    "from pathlib import Path\n",
    "from pprint import pformat\n",
    "\n",
    "import numpy as np\n",
    "import torch.utils.data\n",
    "import wandb\n",
    "from joblib import dump, load\n",
    "# from line_profiler_pycharm import profile\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, confusion_matrix, jaccard_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "from data_utils.MastersDataset import MastersDataset\n",
    "from train_masters import setup_logging_dir, setup_logger, setup_wandb_classification_metrics, _log_string"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-11T22:17:21.398607297Z",
     "start_time": "2023-05-11T22:17:20.444595204Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of  10 | elapsed:    0.9s remaining:    0.9s\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  10 | elapsed:    1.1s remaining:    0.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    1.3s finished\n"
     ]
    }
   ],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import time\n",
    "import random\n",
    "\n",
    "def process_cell(cell_idx):\n",
    "    sleep_time = random.uniform(0.1, 0.5)  # Sleep for a random time between 0.1 and 0.5 seconds\n",
    "    print(f\"Processing cell {cell_idx} - start\")\n",
    "    time.sleep(sleep_time)  # Replace this line with code to process the cell\n",
    "    print(f\"Processing cell {cell_idx} - end\")\n",
    "    return cell_idx\n",
    "\n",
    "# Define the number of parallel processes or threads\n",
    "num_processes = -1\n",
    "\n",
    "# Create a list of cell indices to process\n",
    "cell_indices = list(range(10))  # List of cell indices [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "# Parallelize the loop using joblib\n",
    "results = Parallel(n_jobs=num_processes, verbose=10)(delayed(process_cell)(cell_idx) for cell_idx in cell_indices)\n",
    "\n",
    "print(results)  # Print the results to verify all indices were processed\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-11T22:17:22.730987152Z",
     "start_time": "2023-05-11T22:17:21.403403960Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import random\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed_all(0)\n",
    "random.seed(0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-11T22:17:22.737925333Z",
     "start_time": "2023-05-11T22:17:22.737266004Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def load(path):\n",
    "    s = time.time()\n",
    "    # Setup training/validation data\n",
    "    # TRAIN_DATASET = MastersDataset('train', Path(path), sample_all_points=True)\n",
    "    VAL_DATASET = MastersDataset('validate', Path(path),\n",
    "                                 sample_all_points=True)\n",
    "    # val_data_loader = torch.utils.data.DataLoader(VAL_DATASET, batch_size=1, shuffle=False, num_workers=0)\n",
    "    # CHECK might need to undo this to make getting the variance back easier\n",
    "    # X_train, y_train = TRAIN_DATASET.segment_points[0], TRAIN_DATASET.segment_labels[0]\n",
    "    X_val, y_val = VAL_DATASET.segment_points[0], VAL_DATASET.segment_labels[0]\n",
    "    print(time.time()-s)\n",
    "    return VAL_DATASET"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-11T22:17:22.778322515Z",
     "start_time": "2023-05-11T22:17:22.739540854Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# seq_v = load(\"/home/luc/PycharmProjects/Pointnet_Pointnet2_pytorch/data/PatrickData/Bagni_Nerone/2.5%\")\n",
    "# 3248,"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-11T22:17:22.785487191Z",
     "start_time": "2023-05-11T22:17:22.748962766Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorting by x axis...2.40s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "split y-axis: 100%|██████████| 110/110 [00:03<00:00, 34.14it/s]\n",
      "Fill batches:  93%|█████████▎| 2501/2694 [02:51<00:13, 14.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing cell 0 - start\n",
      "Processing cell 0 - end\n",
      "Processing cell 9 - start\n",
      "Processing cell 9 - end\n",
      "Processing cell 1 - start\n",
      "Processing cell 1 - end\n",
      "Processing cell 4 - start\n",
      "Processing cell 4 - end\n",
      "Processing cell 6 - start\n",
      "Processing cell 6 - end\n",
      "Processing cell 2 - start\n",
      "Processing cell 2 - end\n",
      "Processing cell 5 - start\n",
      "Processing cell 5 - end\n",
      "Processing cell 7 - start\n",
      "Processing cell 7 - end\n",
      "Processing cell 3 - start\n",
      "Processing cell 3 - end\n",
      "Processing cell 8 - start\n",
      "Processing cell 8 - end\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_74332/645666089.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mpar_v\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mload\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"/home/luc/PycharmProjects/Pointnet_Pointnet2_pytorch/data/PatrickData/Bagni_Nerone/2.5%\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m/tmp/ipykernel_74332/2823032962.py\u001B[0m in \u001B[0;36mload\u001B[0;34m(path)\u001B[0m\n\u001B[1;32m      4\u001B[0m     \u001B[0;31m# TRAIN_DATASET = MastersDataset('train', Path(path), sample_all_points=True)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m     VAL_DATASET = MastersDataset('validate', Path(path),\n\u001B[0;32m----> 6\u001B[0;31m                                  sample_all_points=True)\n\u001B[0m\u001B[1;32m      7\u001B[0m     \u001B[0;31m# val_data_loader = torch.utils.data.DataLoader(VAL_DATASET, batch_size=1, shuffle=False, num_workers=0)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      8\u001B[0m     \u001B[0;31m# CHECK might need to undo this to make getting the variance back easier\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/Pointnet_Pointnet2_pytorch/data_utils/MastersDataset.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, split, data_path, num_points_in_block, block_size, sample_all_points, force_even, relative_coords, npy_array)\u001B[0m\n\u001B[1;32m    223\u001B[0m                 \u001B[0mgrid_cell_to_segment\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlabel_batch\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    224\u001B[0m                 \u001B[0;31m# Stack all the points/labels from this cell with the previous cells\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 225\u001B[0;31m                 \u001B[0mdata_segment\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvstack\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mdata_segment\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdata_batch\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0mdata_segment\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msize\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0mdata_batch\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    226\u001B[0m                 \u001B[0mlabels_segment\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mhstack\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mlabels_segment\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlabel_batch\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0mlabels_segment\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msize\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0mlabel_batch\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    227\u001B[0m                 sample_weight_segment = np.hstack(\n",
      "\u001B[0;32m<__array_function__ internals>\u001B[0m in \u001B[0;36mvstack\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/Pointnet_Pointnet2_pytorch/lib/python3.7/site-packages/numpy/core/shape_base.py\u001B[0m in \u001B[0;36mvstack\u001B[0;34m(tup)\u001B[0m\n\u001B[1;32m    280\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0marrs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    281\u001B[0m         \u001B[0marrs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0marrs\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 282\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0m_nx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconcatenate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0marrs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    283\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    284\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<__array_function__ internals>\u001B[0m in \u001B[0;36mconcatenate\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "par_v = load(\"/home/luc/PycharmProjects/Pointnet_Pointnet2_pytorch/data/PatrickData/Bagni_Nerone/2.5%\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-11T22:20:37.456158992Z",
     "start_time": "2023-05-11T22:17:22.755366067Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
