{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-05-11T21:53:42.178073413Z",
     "start_time": "2023-05-11T21:53:41.220593011Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "from time import sleep\n",
    "\n",
    "import time\n",
    "# import pptk\n",
    "from pathlib import Path\n",
    "from pprint import pformat\n",
    "\n",
    "import numpy as np\n",
    "import torch.utils.data\n",
    "import wandb\n",
    "from joblib import dump, load\n",
    "# from line_profiler_pycharm import profile\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, confusion_matrix, jaccard_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "from data_utils.MastersDataset import MastersDataset\n",
    "from train_masters import setup_logging_dir, setup_logger, setup_wandb_classification_metrics, _log_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of  10 | elapsed:    1.0s remaining:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  10 | elapsed:    1.2s remaining:    0.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    1.7s finished\n"
     ]
    }
   ],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import time\n",
    "import random\n",
    "\n",
    "def process_cell(cell_idx):\n",
    "    sleep_time = random.uniform(0.1, 0.5)  # Sleep for a random time between 0.1 and 0.5 seconds\n",
    "    print(f\"Processing cell {cell_idx} - start\")\n",
    "    time.sleep(sleep_time)  # Replace this line with code to process the cell\n",
    "    print(f\"Processing cell {cell_idx} - end\")\n",
    "    return cell_idx\n",
    "\n",
    "# Define the number of parallel processes or threads\n",
    "num_processes = -1\n",
    "\n",
    "# Create a list of cell indices to process\n",
    "cell_indices = list(range(10))  # List of cell indices [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "# Parallelize the loop using joblib\n",
    "results = Parallel(n_jobs=num_processes, verbose=10)(delayed(process_cell)(cell_idx) for cell_idx in cell_indices)\n",
    "\n",
    "print(results)  # Print the results to verify all indices were processed\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-11T21:50:47.960299251Z",
     "start_time": "2023-05-11T21:50:46.199340654Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import random\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed_all(0)\n",
    "random.seed(0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-11T21:53:44.338345184Z",
     "start_time": "2023-05-11T21:53:44.333042742Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def load(path):\n",
    "    s = time.time()\n",
    "    # Setup training/validation data\n",
    "    # TRAIN_DATASET = MastersDataset('train', Path(path), sample_all_points=True)\n",
    "    VAL_DATASET = MastersDataset('validate', Path(path),\n",
    "                                 sample_all_points=True)\n",
    "    # val_data_loader = torch.utils.data.DataLoader(VAL_DATASET, batch_size=1, shuffle=False, num_workers=0)\n",
    "    # CHECK might need to undo this to make getting the variance back easier\n",
    "    # X_train, y_train = TRAIN_DATASET.segment_points[0], TRAIN_DATASET.segment_labels[0]\n",
    "    X_val, y_val = VAL_DATASET.segment_points[0], VAL_DATASET.segment_labels[0]\n",
    "    print(time.time()-s)\n",
    "    return VAL_DATASET"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-11T21:53:44.463353441Z",
     "start_time": "2023-05-11T21:53:44.455919955Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorting by x axis...2.46s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "split y-axis: 100%|██████████| 110/110 [00:02<00:00, 36.85it/s]\n",
      "Fill batches: 100%|██████████| 2694/2694 [03:23<00:00, 13.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "228.2699944972992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# seq_v = load(\"/home/luc/PycharmProjects/Pointnet_Pointnet2_pytorch/data/PatrickData/Bagni_Nerone/2.5%\")\n",
    "# 3248,"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-11T21:27:13.657295686Z",
     "start_time": "2023-05-11T21:23:25.383708658Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorting by x axis...2.36s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "split y-axis: 100%|██████████| 110/110 [00:03<00:00, 30.97it/s]\n",
      "Fill batches:   0%|          | 0/2694 [00:00<?, ?it/s][Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "Fill batches:   0%|          | 8/2694 [00:00<02:22, 18.78it/s][Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:    0.5s\n",
      "Fill batches:   1%|          | 16/2694 [00:00<01:19, 33.54it/s][Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1849s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.0726s.) Setting batch_size=4.\n",
      "Fill batches:   1%|▏         | 36/2694 [00:00<00:35, 75.54it/s][Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.0839s.) Setting batch_size=8.\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:    0.8s\n",
      "Fill batches:   4%|▎         | 100/2694 [00:00<00:13, 191.67it/s][Parallel(n_jobs=-1)]: Batch computation too fast (0.1775s.) Setting batch_size=16.\n",
      "Fill batches:   7%|▋         | 196/2694 [00:01<00:07, 312.84it/s][Parallel(n_jobs=-1)]: Done 116 tasks      | elapsed:    1.0s\n",
      "Fill batches:  12%|█▏        | 324/2694 [00:01<00:07, 302.42it/s][Parallel(n_jobs=-1)]: Done 276 tasks      | elapsed:    1.7s\n",
      "Fill batches:  19%|█▉        | 516/2694 [00:02<00:06, 319.04it/s][Parallel(n_jobs=-1)]: Done 452 tasks      | elapsed:    2.2s\n",
      "Fill batches:  26%|██▋       | 708/2694 [00:02<00:06, 318.01it/s][Parallel(n_jobs=-1)]: Done 660 tasks      | elapsed:    2.9s\n",
      "Fill batches:  36%|███▌      | 964/2694 [00:03<00:05, 297.76it/s][Parallel(n_jobs=-1)]: Done 868 tasks      | elapsed:    3.6s\n",
      "Fill batches:  43%|████▎     | 1156/2694 [00:04<00:05, 291.05it/s][Parallel(n_jobs=-1)]: Done 1108 tasks      | elapsed:    4.4s\n",
      "Fill batches:  52%|█████▏    | 1412/2694 [00:05<00:04, 298.43it/s][Parallel(n_jobs=-1)]: Done 1348 tasks      | elapsed:    5.3s\n",
      "Fill batches:  62%|██████▏   | 1668/2694 [00:06<00:03, 258.34it/s][Parallel(n_jobs=-1)]: Done 1620 tasks      | elapsed:    6.4s\n",
      "Fill batches:  74%|███████▍  | 1988/2694 [00:07<00:02, 299.28it/s][Parallel(n_jobs=-1)]: Done 1892 tasks      | elapsed:    7.2s\n",
      "Fill batches:  83%|████████▎ | 2244/2694 [00:07<00:01, 336.20it/s][Parallel(n_jobs=-1)]: Done 2196 tasks      | elapsed:    8.1s\n",
      "Fill batches:  95%|█████████▌| 2564/2694 [00:08<00:00, 306.61it/s][Parallel(n_jobs=-1)]: Done 2500 tasks      | elapsed:    9.1s\n",
      "Fill batches: 100%|██████████| 2694/2694 [00:09<00:00, 286.08it/s]\n",
      "[Parallel(n_jobs=-1)]: Done 2694 out of 2694 | elapsed:    9.7s finished\n",
      "/home/luc/PycharmProjects/Pointnet_Pointnet2_pytorch/data_utils/MastersDataset.py:236: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  data_segment = np.array(data_segment)\n",
      "/home/luc/PycharmProjects/Pointnet_Pointnet2_pytorch/data_utils/MastersDataset.py:237: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  labels_segment = np.array(labels_segment)\n",
      "/home/luc/PycharmProjects/Pointnet_Pointnet2_pytorch/data_utils/MastersDataset.py:238: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  sample_weight_segment = np.array(sample_weight_segment)\n",
      "/home/luc/PycharmProjects/Pointnet_Pointnet2_pytorch/data_utils/MastersDataset.py:239: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  point_idxs_segment = np.array(point_idxs_segment)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2694\n",
      "(2694,)\n",
      "(2694,)\n",
      "(2694,)\n",
      "(2694,)\n",
      "(2694,)\n",
      "112.85596871376038\n"
     ]
    }
   ],
   "source": [
    "par_v = load(\"/home/luc/PycharmProjects/Pointnet_Pointnet2_pytorch/data/PatrickData/Bagni_Nerone/2.5%\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
